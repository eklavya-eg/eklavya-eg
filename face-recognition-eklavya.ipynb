{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7514479,"sourceType":"datasetVersion","datasetId":4376972}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install cloud-tpu-client==0.10 torch==1.12.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl\n# import torch\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device = xm.xla_device()\n# print(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:52:07.659892Z","iopub.execute_input":"2024-02-10T19:52:07.660784Z","iopub.status.idle":"2024-02-10T19:52:07.664635Z","shell.execute_reply.started":"2024-02-10T19:52:07.660752Z","shell.execute_reply":"2024-02-10T19:52:07.663635Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\n\nclass DataBuilder:\n    def __init__(self, in_path, labels_72, labels_5):\n        self.data = []\n        for ldir in os.listdir(in_path):\n            total = 0\n            if \"72\" in ldir : limit = labels_72\n            elif \"5\" in ldir : limit = labels_5\n            for individuals in os.listdir(os.path.join(in_path, ldir)):\n                if total<limit:\n                    labels = os.listdir(os.path.join(in_path, ldir))\n                    labels = list(set(labels)-{individuals})\n                    random.shuffle(labels)\n                    images = os.listdir(os.path.join(in_path, ldir, individuals))\n                    anchor = \"0.png\"\n                    images = list(set(images)-{anchor})\n                    anchor = \"/\".join([in_path, ldir, individuals, anchor])\n                    for i in range(len(images)):\n                        pos = \"/\".join([in_path, ldir, individuals, images[i]])\n                        if i != 0:\n                            label = os.listdir(os.path.join(in_path, ldir, labels[i]))\n                            random.shuffle(label)\n                            label = label[0]\n                            neg = \"/\".join([in_path, ldir, labels[i], label])\n                        else:\n                            label = os.listdir(os.path.join(in_path, ldir, labels[i]))[0]\n                            neg = \"/\".join([in_path, ldir, labels[i], label])\n                        self.data.append([anchor, pos, neg])\n                    total += 1\n\n\npath = \"/kaggle/input/face-recognition\"\n# path = \"/\".join(path)\ndata = DataBuilder(path, 1000, 1000)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:52:07.668867Z","iopub.execute_input":"2024-02-10T19:52:07.669162Z","iopub.status.idle":"2024-02-10T19:54:05.027770Z","shell.execute_reply.started":"2024-02-10T19:52:07.669141Z","shell.execute_reply":"2024-02-10T19:54:05.026797Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random\ndata = data.data\nrandom.shuffle(data)\nfor i in data[:5]:\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:54:05.029948Z","iopub.execute_input":"2024-02-10T19:54:05.030664Z","iopub.status.idle":"2024-02-10T19:54:05.099799Z","shell.execute_reply.started":"2024-02-10T19:54:05.030626Z","shell.execute_reply":"2024-02-10T19:54:05.098936Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['/kaggle/input/face-recognition/subjects_0-1999_72_imgs/503/0.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/503/35.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/962/28.png']\n['/kaggle/input/face-recognition/subjects_0-1999_72_imgs/32/0.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/32/62.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/27/45.png']\n['/kaggle/input/face-recognition/subjects_0-1999_72_imgs/857/0.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/857/60.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/228/3.png']\n['/kaggle/input/face-recognition/subjects_0-1999_72_imgs/405/0.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/405/62.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/700/53.png']\n['/kaggle/input/face-recognition/subjects_0-1999_72_imgs/442/0.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/442/53.png', '/kaggle/input/face-recognition/subjects_0-1999_72_imgs/1291/27.png']\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_image, ImageReadMode\nfrom sklearn.model_selection import train_test_split\n\nclass FaceDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __getitem__(self, index):\n        anchor, positive, negative = self.data[index]\n        anchor = read_image(anchor, mode=ImageReadMode.RGB)\n        positive = read_image(positive, mode=ImageReadMode.RGB)\n        negative = read_image(negative, mode=ImageReadMode.RGB)\n        return torch.as_tensor(anchor, dtype=torch.float32), torch.as_tensor(positive, dtype=torch.float32), torch.as_tensor(negative, dtype=torch.float32)\n    \n    def __len__(self):\n        return len(self.data)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:54:05.100825Z","iopub.execute_input":"2024-02-10T19:54:05.101161Z","iopub.status.idle":"2024-02-10T19:54:11.522895Z","shell.execute_reply.started":"2024-02-10T19:54:05.101130Z","shell.execute_reply":"2024-02-10T19:54:11.521944Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn as nn\nfrom torch import optim as optim\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\n\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, embedding_size, p_dropout, p_linear_dropout):\n        super(Model, self).__init__()\n        self.conv = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0),\n                                   nn.AvgPool2d(kernel_size=2, stride=2))\n        \n\n        self.b1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(64),\n                                nn.LeakyReLU(0.1))\n        self.b2 = nn.Sequential(nn.Conv2d(in_channels, 92, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(92),\n                                nn.LeakyReLU(0.1),\n                                nn.Conv2d(92, 128, kernel_size=3, padding=\"same\"),\n                                nn.BatchNorm2d(128),\n                                nn.LeakyReLU(0.1),)\n        self.b3 = nn.Sequential(nn.Conv2d(in_channels, 24, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(24),\n                                nn.LeakyReLU(0.1),\n                                nn.Conv2d(24, 32, kernel_size=5, padding=\"same\"),\n                                nn.BatchNorm2d(32),\n                                nn.LeakyReLU(0.1))\n        self.b4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, padding=1, stride = 1),\n                                nn.Conv2d(in_channels, 32, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(32),\n                                nn.LeakyReLU(0.1))\n        \n\n        self.b5 = nn.Sequential(nn.Conv2d(256, 32, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(32),\n                                nn.LeakyReLU(0.1))\n        self.b6 = nn.Sequential(nn.Conv2d(256, 128, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(128),\n                                nn.LeakyReLU(0.1),\n                                nn.Conv2d(128, 64, kernel_size=3, padding=\"same\"),\n                                nn.BatchNorm2d(64),\n                                nn.LeakyReLU(0.1),)\n        self.b7 = nn.Sequential(nn.Conv2d(256, 24, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(24),\n                                nn.LeakyReLU(0.1),\n                                nn.Conv2d(24, 16, kernel_size=5, padding=\"same\"),\n                                nn.BatchNorm2d(16),\n                                nn.LeakyReLU(0.1))\n        self.b8 = nn.Sequential(nn.MaxPool2d(kernel_size=3, padding=1, stride = 1),\n                                nn.Conv2d(256, 16, kernel_size=1, padding=\"same\"),\n                                nn.BatchNorm2d(16),\n                                nn.LeakyReLU(0.1))\n\n        self.dropout = nn.Dropout2d(p_dropout)\n        \n        self.fc = nn.Sequential(nn.Linear(401408, 512, bias=True),\n                                nn.BatchNorm1d(512),\n                                nn.LeakyReLU(0.1),\n                                nn.Dropout(p_linear_dropout),\n                                nn.Linear(512, embedding_size, bias=True))\n        \n        \n    def forward(self, x, batch_size):\n        x = self.conv(x)\n\n        xb1 = self.b1(x)\n        xb2 = self.b2(x)\n        xb3 = self.b3(x)\n        xb4 = self.b4(x)\n        x = torch.cat([xb1, xb2, xb3, xb4], dim = 1)\n\n        x = self.dropout(x)\n\n        xb5 = self.b5(x)\n        xb6 = self.b6(x)\n        xb7 = self.b7(x)\n        xb8 = self.b8(x)\n        x = torch.cat([xb5, xb6, xb7, xb8], dim = 1)\n\n        x = self.dropout(x)\n\n        \n        x = x.view(batch_size, -1)\n        x = self.fc(x)\n\n        return x\n\nclass TripletLoss:\n    def __init__(self, margin, p):\n        self.margin = margin\n        self.p = p\n\n    def loss(self, anchor, positive, negative):\n        distance_ap = F.pairwise_distance(anchor, positive, p = self.p)\n        distance_an = F.pairwise_distance(anchor, negative, p = self.p)\n        tripletloss = torch.clamp(input=distance_ap+(self.margin)-distance_an, min=0.0).mean()\n        return tripletloss\n\nclass Optimizer:\n    def __init__(self, model, lr):\n        self.model = model\n        self.lr = lr\n    \n    def optimize(self):\n        return optim.Adam(self.model.parameters(), lr=self.lr)\n\nclass Schedular:\n    def __init__(self, optimizer, lr, step_size, gamma):\n        self.optimizer = optimizer\n        self.lr = lr\n        self.step_size = step_size\n        self.gamma = gamma\n    \n    def decay(self):\n        return lr_scheduler.StepLR(self.optimizer, self.step_size, self.gamma)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:54:11.525199Z","iopub.execute_input":"2024-02-10T19:54:11.525621Z","iopub.status.idle":"2024-02-10T19:54:11.550141Z","shell.execute_reply.started":"2024-02-10T19:54:11.525591Z","shell.execute_reply":"2024-02-10T19:54:11.549239Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Variables:\n    def __init__(self, in_channels=3, embedding_size=256, batch_size=40, batch_size_test_val = 40, lr=0.001, margin=0.3, p=2, epochs=100, step_size=3, gamma=0.5, p_dropout=0.2, p_linear_dropout=0.2):\n        self.batch_size = batch_size\n        self.lr = lr\n        self.in_channels = in_channels\n        self.emembedding_size =embedding_size\n        self.p = p\n        self.margin = margin\n        self.epochs = epochs\n        self.step_size = step_size\n        self.gamma = gamma\n        self.batch_size_test_val = batch_size_test_val\n        self.p_dropout = p_dropout\n        self.p_linear_dropout = p_linear_dropout","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:54:11.551261Z","iopub.execute_input":"2024-02-10T19:54:11.551620Z","iopub.status.idle":"2024-02-10T19:54:11.563713Z","shell.execute_reply.started":"2024-02-10T19:54:11.551588Z","shell.execute_reply":"2024-02-10T19:54:11.562854Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn as nn\nfrom torch import optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\n\nif __name__ == \"__main__\":\n    # warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.init\")\n\n    train = data[0:70000]\n    val = data[67000:72000]\n    test = data[70000:75000]\n\n    train = FaceDataset(train)\n    val = FaceDataset(val)\n    test = FaceDataset(test)\n    variables = Variables()\n    train = DataLoader(train, batch_size = variables.batch_size, shuffle=False)\n    val = DataLoader(val, batch_size = variables.batch_size_test_val, shuffle=False)\n    test = DataLoader(test, batch_size = variables.batch_size_test_val, shuffle=False)\n\n    print(\"train:\", len(train))\n    print(\"val:\", len(val))\n    print(\"test:\", len(test))\n    \n    model = Model(in_channels=variables.in_channels, embedding_size=variables.emembedding_size, p_dropout=variables.p_dropout, p_linear_dropout=variables.p_linear_dropout)\n    tripletloss = TripletLoss(margin=variables.margin, p=variables.p)\n    optimizer = Optimizer(model=model, lr=variables.lr).optimize()\n    schedular = Schedular(optimizer, variables.lr, variables.step_size, variables.gamma).decay()\n    device = \"cuda\"\n    # device = xm.xla_device()\n    model.to(device)\n\n    for epoch in range(0, variables.epochs):\n        model.train()\n        train_cost = 0.0\n        loop = tqdm(enumerate(train), total=len(train), leave=True)\n        for _, train_batch in loop:\n            anc, pos, neg = train_batch\n            anc, pos, neg = anc.to(device), pos.to(device), neg.to(device)\n            anc = model.forward(anc, variables.batch_size)\n            pos = model.forward(pos, variables.batch_size)\n            neg = model.forward(neg, variables.batch_size)\n            loss = tripletloss.loss(anc, pos, neg)\n            train_cost += loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            loop.set_description(f\"[{epoch+1}/{variables.epochs}]\")\n            loop.set_postfix(loss=loss.item())\n        avg_train_loss = train_cost/len(train)\n        \n        model.eval()\n        with torch.no_grad():\n            val_cost = 0.0\n            loop_val = tqdm(enumerate(val), total=len(val), leave=True)\n            for _, val_batch in loop_val:\n                anc, pos, neg = val_batch\n                anc, pos, neg = anc.to(device), pos.to(device), neg.to(device)\n                anc = model.forward(anc, variables.batch_size_test_val)\n                pos = model.forward(pos, variables.batch_size_test_val)\n                neg = model.forward(neg, variables.batch_size_test_val)\n                val_loss = tripletloss.loss(anc, pos, neg)\n                val_cost += val_loss.item()\n                loop_val.set_description(f\"[{epoch+1}/{variables.epochs}]\")\n                loop_val.set_postfix(loss=val_loss.item())\n            avg_val_loss = val_cost/len(val)\n            print(f\"\\nEpoch {epoch+1}/{variables.epochs}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n        checkpoint_filename = f\"model_epoch_{epoch+1}.pth\"\n        if avg_train_loss < 1:\n            if avg_val_loss < 1:\n                torch.save(model.state_dict(), checkpoint_filename)\n        elif (epoch+1)%5==0:\n            torch.save(model.state_dict(), checkpoint_filename)\n        schedular.step()\n        print(\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-10T19:54:11.565177Z","iopub.execute_input":"2024-02-10T19:54:11.565493Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"train: 1750\nval: 125\ntest: 125\n","output_type":"stream"},{"name":"stderr","text":"[1/100]: 100%|██████████| 1750/1750 [17:15<00:00,  1.69it/s, loss=0.328]\n[1/100]: 100%|██████████| 125/125 [00:41<00:00,  3.03it/s, loss=0.402]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/100, Training Loss: 0.40805330872535706, Validation Loss: 0.2013580079227686\n\n\n","output_type":"stream"},{"name":"stderr","text":"[2/100]: 100%|██████████| 1750/1750 [15:01<00:00,  1.94it/s, loss=0.105]  \n[2/100]: 100%|██████████| 125/125 [00:40<00:00,  3.09it/s, loss=0.207] \n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/100, Training Loss: 0.22244086861610413, Validation Loss: 0.09741024102270603\n\n\n","output_type":"stream"},{"name":"stderr","text":"[3/100]:  83%|████████▎ | 1455/1750 [11:44<02:19,  2.11it/s, loss=0.0916] ","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), \"abcd.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nxm.rendezvous('clear_memory')  # This can help clear TPU memory\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}